---
title: Module 4 - Vision-Language-Action (VLA)
sidebar_label: Module 4 Overview
description: Introduction to Vision-Language-Action models for robotics
---

# Module 4: Vision-Language-Action (VLA)

Welcome to Module 4 of the Physical AI & Humanoid Robotics Learning Platform. In this module, you'll explore Vision-Language-Action (VLA) models that enable robots to understand and execute complex commands based on visual and linguistic inputs.

## What You'll Learn

- Vision-Language-Action model fundamentals
- Voice command interpretation and action execution
- Cognitive planning for robotic systems
- Edge cases and error handling in VLA systems
- Capstone project integrating all previous modules
- Validation techniques for VLA systems

## Prerequisites

- Understanding of ROS 2 (from Module 1)
- Knowledge of digital twins (from Module 2)
- Experience with NVIDIA Isaac (from Module 3)
- Familiarity with AI concepts

## Module Structure

- Chapter 1: Voice to Action
- Chapter 2: Cognitive Planning
- Chapter 3: Capstone Project
- Additional Resources: Edge Cases, Validation, and Quickstart

## Getting Started

Start with [Quickstart](./quickstart.md) to get an overview, then proceed to [Chapter 1: Voice to Action](./chapter-1-voice-to-action.md) to begin your learning journey.